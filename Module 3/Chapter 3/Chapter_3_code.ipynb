{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Fast Learning SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory is: \"C:\\scisoft\\WinPython-64bit-2.7.9.4\\notebooks\\Packt - Large Scale CHECK\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print \"Current directory is: \\\"%s\\\"\" % (os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib2 # import urllib.request as urllib2 in Python3\n",
    "import requests, io, os, StringIO\n",
    "import numpy as np\n",
    "import tarfile, zipfile, gzip\n",
    "\n",
    "def unzip_from_UCI(UCI_url, dest=''):\n",
    "    \"\"\"\n",
    "    Downloads and unpacks datasets from UCI in zip format\n",
    "    \"\"\"\n",
    "    response = requests.get(UCI_url)\n",
    "    compressed_file = io.BytesIO(response.content)\n",
    "    z = zipfile.ZipFile(compressed_file)\n",
    "    print ('Extracting in %s' %  os.getcwd()+'\\\\'+dest)\n",
    "    for name in z.namelist():\n",
    "        if '.csv' in name:\n",
    "            print ('\\tunzipping %s' %name)\n",
    "            z.extract(name, path=os.getcwd()+'\\\\'+dest)\n",
    "\n",
    "def gzip_from_UCI(UCI_url, dest=''):\n",
    "    \"\"\"\n",
    "    Downloads and unpacks datasets from UCI in gzip format\n",
    "    \"\"\"\n",
    "    response = urllib2.urlopen(UCI_url)\n",
    "    compressed_file = io.BytesIO(response.read())\n",
    "    decompressed_file = gzip.GzipFile(fileobj=compressed_file)\n",
    "    filename = UCI_url.split('/')[-1][:-3]\n",
    "    with open(os.getcwd()+'\\\\'+filename, 'wb') as outfile:\n",
    "        outfile.write(decompressed_file.read())\n",
    "    print ('File %s decompressed' % filename)\n",
    "            \n",
    "def targzip_from_UCI(UCI_url, dest='.'):\n",
    "    \"\"\"\n",
    "    Downloads and unpacks datasets from UCI in tar.gz format\n",
    "    \"\"\"\n",
    "    response = urllib2.urlopen(UCI_url)\n",
    "    compressed_file = StringIO.StringIO(response.read())\n",
    "    tar = tarfile.open(mode=\"r:gz\", fileobj = compressed_file)\n",
    "    tar.extractall(path=dest)\n",
    "    datasets = tar.getnames()\n",
    "    for dataset in datasets:\n",
    "        size = os.path.getsize(dest+'\\\\'+dataset)\n",
    "        print ('File %s is %i bytes' % (dataset,size))\n",
    "    tar.close()\n",
    "\n",
    "def load_matrix(UCI_url):\n",
    "    \"\"\"\n",
    "    Downloads datasets from UCI in matrix form\n",
    "    \"\"\"\n",
    "    return np.loadtxt(urllib2.urlopen(UCI_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Bike Sharing Dataset Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting in C:\\scisoft\\WinPython-64bit-2.7.9.4\\notebooks\\Packt - Large Scale CHECK\\bikesharing\n",
      "\tunzipping day.csv\n",
      "\tunzipping hour.csv\n"
     ]
    }
   ],
   "source": [
    "UCI_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00275/Bike-Sharing-Dataset.zip'\n",
    "unzip_from_UCI(UCI_url, dest='bikesharing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Covertype Data Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File covtype.data decompressed\n"
     ]
    }
   ],
   "source": [
    "UCI_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz'\n",
    "gzip_from_UCI(UCI_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Understanding Scikit-learn SVM implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X_i, y_i = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.969\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import numpy as np\n",
    "h_class = SVC(kernel='rbf', C=1.0, gamma=0.7, random_state=101)\n",
    "scores = cross_val_score(h_class, X_i, y_i, cv=20, scoring='accuracy')\n",
    "print 'Accuracy: %0.3f' % np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 13  14  15  22  24  41  44  50  52  56  60  62  63  66  68  70  72  76\n",
      "  77  83  84  85  98 100 106 110 114 117 118 119 121 123 126 127 129 131\n",
      " 133 134 138 141 146 149]\n"
     ]
    }
   ],
   "source": [
    "h_class.fit(X_i,y_i)\n",
    "print h_class.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "boston = load_boston()\n",
    "shuffled = np.random.permutation(boston.target.size)\n",
    "X_b = scaler.fit_transform(boston.data[shuffled,:])\n",
    "y_b = boston.target[shuffled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 28.218\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "h_regr = SVR(kernel='rbf', C=20.0, gamma=0.001, epsilon=1.0)\n",
    "scores = cross_val_score(h_regr, X_b, y_b, cv=20, scoring='mean_squared_error')\n",
    "print 'Mean Squared Error: %0.3f' % abs(np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pursuing non linear SVM by sub-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import seed, randint\n",
    "SAMPLE_COUNT = 5000\n",
    "TEST_COUNT   = 20000\n",
    "seed(0) # allows repeatable results\n",
    "sample = list()\n",
    "test_sample = list()\n",
    "for index, line in enumerate(open('covtype.data','rb')):\n",
    "    if index < SAMPLE_COUNT:\n",
    "        sample.append(line)\n",
    "    else:\n",
    "        r = randint(0, index)\n",
    "        if r < SAMPLE_COUNT:\n",
    "            sample[r] = line\n",
    "        else:\n",
    "            k = randint(0, index)\n",
    "            if k < TEST_COUNT:\n",
    "                if len(test_sample) < TEST_COUNT:\n",
    "                    test_sample.append(line)\n",
    "                else:\n",
    "                    test_sample[k] = line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "for n,line in enumerate(sample):\n",
    "        sample[n] = map(float,line.strip().split(','))\n",
    "y = np.array(sample)[:,-1]\n",
    "scaling = StandardScaler()\n",
    "X = scaling.fit_transform(np.array(sample)[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for n,line in enumerate(test_sample):\n",
    "        test_sample[n] = map(float,line.strip().split(','))\n",
    "yt = np.array(test_sample)[:,-1]\n",
    "Xt = scaling.transform(np.array(test_sample)[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75205\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "h = SVC(kernel='rbf', C=250.0, gamma=0.0025, random_state=101)\n",
    "h.fit(X,y)\n",
    "prediction = h.predict(Xt)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print accuracy_score(yt, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Achieving SVM at scale with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv, time, os\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def explore(target_file, separator=',', fieldnames= None, binary_features=list(), numeric_features=list(), max_rows=20000):\n",
    "    \"\"\"\n",
    "    Generate from an online style stream a DictVectorizer and a MinMaxScaler.\n",
    "    \n",
    "    Parameters\n",
    "    ‐‐‐‐‐‐‐‐‐‐\n",
    "    target file = the file to stream from\n",
    "    separator = the field separator character\n",
    "    fieldnames = the fields' labels (can be ommitted and read from file)\n",
    "    binary_features = the list of qualitative features to consider\n",
    "    numeric_features = the list of numeric futures to consider\n",
    "    max_rows = the number of rows to be read from the stream (can be None)\n",
    "    \"\"\"\n",
    "    features = dict()\n",
    "    min_max  = dict()\n",
    "    vectorizer = DictVectorizer(sparse=False)\n",
    "    scaler = MinMaxScaler()\n",
    "    with open(target_file, 'rb') as R:\n",
    "        iterator = csv.DictReader(R, fieldnames, delimiter=separator)\n",
    "        for n, row in enumerate(iterator):\n",
    "            # DATA EXPLORATION\n",
    "            for k,v in row.iteritems():\n",
    "                if k in binary_features:\n",
    "                    if k+'_'+v not in features:\n",
    "                        features[k+'_'+v]=0\n",
    "                elif k in numeric_features:\n",
    "                    v = float(v)\n",
    "                    if k not in features:\n",
    "                        features[k]=0\n",
    "                        min_max[k] = [v,v]\n",
    "                    else:\n",
    "                        if v < min_max[k][0]:\n",
    "                            min_max[k][0]= v\n",
    "                        elif v > min_max[k][1]:\n",
    "                            min_max[k][1]= v\n",
    "                else:\n",
    "                    pass # ignore the feature\n",
    "            if max_rows and n > max_rows:\n",
    "                break\n",
    "    vectorizer.fit([features])\n",
    "    A = vectorizer.transform([{f:0 if f not in min_max else min_max[f][0] for f in vectorizer.feature_names_},\n",
    "{f:1 if f not in min_max else min_max[f][1] for f in vectorizer.feature_names_}])\n",
    "    scaler.fit(A)\n",
    "    return vectorizer, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pull_examples(target_file, vectorizer, binary_features, numeric_features, target, min_max=None, separator=',', \n",
    "fieldnames=None, sparse=True):\n",
    "    \"\"\"\n",
    "    Reads a online style stream and returns a generator of normalized feature vectors\n",
    "    \n",
    "    Parameters\n",
    "    ‐‐‐‐‐‐‐‐‐‐\n",
    "    target file = the file to stream from\n",
    "    vectorizer = a DictVectorizer object\n",
    "    binary_features = the list of qualitative features to consider\n",
    "    numeric_features = the list of numeric features to consider\n",
    "    target = the label of the response variable\n",
    "    min_max = a MinMaxScaler object, can be omitted leaving None\n",
    "    separator = the field separator character\n",
    "    fieldnames = the fields' labels (can be ommitted and read from file)\n",
    "    sparse = if a sparse vector is to be returned from the generator\n",
    "    \"\"\"\n",
    "    with open(target_file, 'rb') as R:\n",
    "        iterator = csv.DictReader(R, fieldnames, delimiter=separator)\n",
    "        for n, row in enumerate(iterator):\n",
    "            # DATA PROCESSING\n",
    "            stream_row = {}\n",
    "            response = np.array([float(row[target])])\n",
    "            for k,v in row.iteritems():\n",
    "                if k in binary_features:\n",
    "                    stream_row[k+'_'+v]=1.0 \n",
    "                else:\n",
    "                    if k in numeric_features:\n",
    "                        stream_row[k]=float(v)\n",
    "            if min_max:\n",
    "                features = min_max.transform(vectorizer.transform([stream_row]))\n",
    "            else:\n",
    "                features = vectorizer.transform([stream_row])\n",
    "            if sparse:\n",
    "                yield(csr_matrix(features), response, n)\n",
    "            else:\n",
    "                yield(features, response, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: \n",
      "atemp:[0.00,1.00] \n",
      "holiday_0:[0.00,1.00] \n",
      "holiday_1:[0.00,1.00] \n",
      "hr_0:[0.00,1.00] \n",
      "hr_1:[0.00,1.00] \n",
      "hr_10:[0.00,1.00] \n",
      "hr_11:[0.00,1.00] \n",
      "hr_12:[0.00,1.00] \n",
      "hr_13:[0.00,1.00] \n",
      "hr_14:[0.00,1.00] \n",
      "hr_15:[0.00,1.00] \n",
      "hr_16:[0.00,1.00] \n",
      "hr_17:[0.00,1.00] \n",
      "hr_18:[0.00,1.00] \n",
      "hr_19:[0.00,1.00] \n",
      "hr_2:[0.00,1.00] \n",
      "hr_20:[0.00,1.00] \n",
      "hr_21:[0.00,1.00] \n",
      "hr_22:[0.00,1.00] \n",
      "hr_23:[0.00,1.00] \n",
      "hr_3:[0.00,1.00] \n",
      "hr_4:[0.00,1.00] \n",
      "hr_5:[0.00,1.00] \n",
      "hr_6:[0.00,1.00] \n",
      "hr_7:[0.00,1.00] \n",
      "hr_8:[0.00,1.00] \n",
      "hr_9:[0.00,1.00] \n",
      "hum:[0.00,1.00] \n",
      "mnth_1:[0.00,1.00] \n",
      "mnth_10:[0.00,1.00] \n",
      "mnth_11:[0.00,1.00] \n",
      "mnth_12:[0.00,1.00] \n",
      "mnth_2:[0.00,1.00] \n",
      "mnth_3:[0.00,1.00] \n",
      "mnth_4:[0.00,1.00] \n",
      "mnth_5:[0.00,1.00] \n",
      "mnth_6:[0.00,1.00] \n",
      "mnth_7:[0.00,1.00] \n",
      "mnth_8:[0.00,1.00] \n",
      "mnth_9:[0.00,1.00] \n",
      "season_1:[0.00,1.00] \n",
      "season_2:[0.00,1.00] \n",
      "season_3:[0.00,1.00] \n",
      "season_4:[0.00,1.00] \n",
      "temp:[0.02,1.00] \n",
      "weathersit_1:[0.00,1.00] \n",
      "weathersit_2:[0.00,1.00] \n",
      "weathersit_3:[0.00,1.00] \n",
      "weathersit_4:[0.00,1.00] \n",
      "weekday_0:[0.00,1.00] \n",
      "weekday_1:[0.00,1.00] \n",
      "weekday_2:[0.00,1.00] \n",
      "weekday_3:[0.00,1.00] \n",
      "weekday_4:[0.00,1.00] \n",
      "weekday_5:[0.00,1.00] \n",
      "weekday_6:[0.00,1.00] \n",
      "windspeed:[0.00,0.85] \n",
      "workingday_0:[0.00,1.00] \n",
      "workingday_1:[0.00,1.00] \n",
      "yr_0:[0.00,1.00] \n",
      "yr_1:[0.00,1.00] \n"
     ]
    }
   ],
   "source": [
    "source = '\\\\bikesharing\\\\hour.csv'\n",
    "local_path = os.getcwd()\n",
    "b_vars = ['holiday','hr','mnth', 'season','weathersit','weekday','workingday','yr']\n",
    "n_vars = ['hum', 'temp', 'atemp', 'windspeed']\n",
    "std_row, min_max = explore(target_file=local_path+'\\\\'+source, binary_features=b_vars, numeric_features=n_vars)\n",
    "print 'Features: '\n",
    "for f,mv,mx in zip(std_row.feature_names_, min_max.data_min_, min_max.data_max_):\n",
    "    print '%s:[%0.2f,%0.2f] ' % (f,mv,mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16249 09:42:40 holdout RMSE: 276.604 holdout RMSLE: 1.796\n",
      "16499 09:42:40 holdout RMSE: 250.419 holdout RMSLE: 1.706\n",
      "16749 09:42:41 holdout RMSE: 250.639 holdout RMSLE: 1.694\n",
      "16999 09:42:41 holdout RMSE: 249.561 holdout RMSLE: 1.702\n",
      "17249 09:42:41 holdout RMSE: 234.840 holdout RMSLE: 1.640\n",
      "09:42:41 FINAL holdout RMSE: 224.404\n",
      "09:42:41 FINAL holdout RMSLE: 1.594\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "SGD = SGDRegressor(loss='epsilon_insensitive', epsilon=0.001, penalty=None, random_state=1, average=True)\n",
    "val_rmse = 0\n",
    "val_rmsle = 0\n",
    "predictions_start = 16000\n",
    "\n",
    "def apply_log(x): return np.log(x + 1.0)\n",
    "def apply_exp(x): return np.exp(x) - 1.0\n",
    "\n",
    "for x,y,n in pull_examples(target_file=local_path+'\\\\'+source, \n",
    "                           vectorizer=std_row, min_max=min_max,\n",
    "                           binary_features=b_vars, numeric_features=n_vars, target='cnt'):\n",
    "    y_log = apply_log(y)\n",
    "    # MACHINE LEARNING\n",
    "    if (n+1) >= predictions_start:\n",
    "        # HOLDOUT AFTER N PHASE\n",
    "        predicted = SGD.predict(x)\n",
    "        val_rmse += (apply_exp(predicted) - y)**2\n",
    "        val_rmsle += (predicted - y_log)**2\n",
    "        if (n-predictions_start+1) % 250 == 0 and (n+1) > predictions_start:\n",
    "            print n,\n",
    "            print '%s holdout RMSE: %0.3f' % (time.strftime('%X'), (val_rmse / float(n-predictions_start+1))**0.5),\n",
    "            print 'holdout RMSLE: %0.3f' % ((val_rmsle / float(n-predictions_start+1))**0.5)\n",
    "    else:\n",
    "        # LEARNING PHASE\n",
    "        SGD.partial_fit(x, y_log)\n",
    "print '%s FINAL holdout RMSE: %0.3f' % (time.strftime('%X'), (val_rmse / float(n-predictions_start+1))**0.5)\n",
    "print '%s FINAL holdout RMSLE: %0.3f' % (time.strftime('%X'), (val_rmsle / float(n-predictions_start+1))**0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: \n",
      "var_00:[1872.00,3846.00] \n",
      "var_01:[0.00,360.00] \n",
      "var_02:[0.00,55.00] \n",
      "var_03:[0.00,1369.00] \n",
      "var_04:[-163.00,599.00] \n",
      "var_05:[0.00,7117.00] \n",
      "var_06:[0.00,254.00] \n",
      "var_07:[78.00,254.00] \n",
      "var_08:[0.00,252.00] \n",
      "var_09:[0.00,7172.00] \n",
      "var_10:[0.00,1.00] \n",
      "var_11:[0.00,1.00] \n",
      "var_12:[0.00,1.00] \n",
      "var_13:[0.00,1.00] \n",
      "var_14:[0.00,1.00] \n",
      "var_15:[0.00,1.00] \n",
      "var_16:[0.00,1.00] \n",
      "var_17:[0.00,1.00] \n",
      "var_18:[0.00,1.00] \n",
      "var_19:[0.00,1.00] \n",
      "var_20:[0.00,1.00] \n",
      "var_21:[0.00,1.00] \n",
      "var_22:[0.00,1.00] \n",
      "var_23:[0.00,1.00] \n",
      "var_24:[0.00,1.00] \n",
      "var_25:[0.00,1.00] \n",
      "var_26:[0.00,1.00] \n",
      "var_27:[0.00,1.00] \n",
      "var_28:[0.00,0.00] \n",
      "var_29:[0.00,1.00] \n",
      "var_30:[0.00,1.00] \n",
      "var_31:[0.00,1.00] \n",
      "var_32:[0.00,1.00] \n",
      "var_33:[0.00,1.00] \n",
      "var_34:[0.00,1.00] \n",
      "var_35:[0.00,1.00] \n",
      "var_36:[0.00,1.00] \n",
      "var_37:[0.00,1.00] \n",
      "var_38:[0.00,1.00] \n",
      "var_39:[0.00,1.00] \n",
      "var_40:[0.00,1.00] \n",
      "var_41:[0.00,1.00] \n",
      "var_42:[0.00,1.00] \n",
      "var_43:[0.00,1.00] \n",
      "var_44:[0.00,1.00] \n",
      "var_45:[0.00,1.00] \n",
      "var_46:[0.00,1.00] \n",
      "var_47:[0.00,1.00] \n",
      "var_48:[0.00,1.00] \n",
      "var_49:[0.00,1.00] \n",
      "var_50:[0.00,1.00] \n",
      "var_51:[0.00,1.00] \n",
      "var_52:[0.00,1.00] \n",
      "var_53:[0.00,1.00] \n"
     ]
    }
   ],
   "source": [
    "source = 'shuffled_covtype.data'\n",
    "local_path = os.getcwd()\n",
    "n_vars = ['var_'+'0'*int(j<10)+str(j) for j in range(54)]\n",
    "std_row, min_max = explore(target_file=local_path+'\\\\'+source, binary_features=list(), \n",
    "                  fieldnames= n_vars+['covertype'], numeric_features=n_vars, max_rows=50000)\n",
    "print 'Features: '\n",
    "for f,mv,mx in zip(std_row.feature_names_, min_max.data_min_, min_max.data_max_):\n",
    "    print '%s:[%0.2f,%0.2f] ' % (f,mv,mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:42:57 Progressive accuracy at example 5000: 0.652\n",
      "09:43:08 Progressive accuracy at example 10000: 0.672\n",
      "09:43:18 Progressive accuracy at example 15000: 0.681\n",
      "09:43:29 Progressive accuracy at example 20000: 0.687\n",
      "09:43:39 Progressive accuracy at example 25000: 0.692\n",
      "09:43:50 Progressive accuracy at example 30000: 0.695\n",
      "09:44:01 Progressive accuracy at example 35000: 0.697\n",
      "09:44:11 Progressive accuracy at example 40000: 0.699\n",
      "09:44:22 Progressive accuracy at example 45000: 0.700\n",
      "09:44:32 Progressive accuracy at example 50000: 0.702\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "SGD = SGDClassifier(loss='hinge', penalty=None, random_state=1, average=True)\n",
    "accuracy = 0\n",
    "accuracy_record = list()\n",
    "predictions_start = 50\n",
    "sample = 5000\n",
    "early_stop = 50000\n",
    "for x,y,n in pull_examples(target_file=local_path+'\\\\'+source, \n",
    "                           vectorizer=std_row,\n",
    "                           min_max=min_max,\n",
    "                           binary_features=list(), numeric_features=n_vars,\n",
    "                           fieldnames= n_vars+['covertype'], target='covertype'):\n",
    "    # LEARNING PHASE\n",
    "    if n > predictions_start:\n",
    "        accuracy += int(int(SGD.predict(x))==y[0])\n",
    "        if n % sample == 0:\n",
    "            accuracy_record.append(accuracy / float(sample))\n",
    "            print '%s Progressive accuracy at example %i: %0.3f' % (time.strftime('%X'), n, np.mean(accuracy_record[-sample:]))\n",
    "            accuracy = 0\n",
    "    if early_stop and n >= early_stop:\n",
    "            break\n",
    "    SGD.partial_fit(x, y, classes=range(1,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including non-linearities in SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16249 09:44:45 holdout RMSE: 269.222 holdout RMSLE: 1.627\n",
      "16499 09:44:45 holdout RMSE: 243.626 holdout RMSLE: 1.550\n",
      "16749 09:44:45 holdout RMSE: 244.336 holdout RMSLE: 1.554\n",
      "16999 09:44:45 holdout RMSE: 243.608 holdout RMSLE: 1.570\n",
      "17249 09:44:45 holdout RMSE: 229.319 holdout RMSLE: 1.519\n",
      "09:44:45 FINAL holdout RMSE: 219.191\n",
      "09:44:45 FINAL holdout RMSLE: 1.480\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from  sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "source = '\\\\bikesharing\\\\hour.csv'\n",
    "local_path = os.getcwd()\n",
    "b_vars = ['holiday','hr','mnth', 'season','weathersit','weekday','workingday','yr']\n",
    "n_vars = ['hum', 'temp', 'atemp', 'windspeed']\n",
    "std_row, min_max = explore(target_file=local_path+'\\\\'+source, binary_features=b_vars, numeric_features=n_vars)\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "SGD = SGDRegressor(loss='epsilon_insensitive', epsilon=0.001, penalty=None, random_state=1, average=True)\n",
    "\n",
    "val_rmse = 0\n",
    "val_rmsle = 0\n",
    "predictions_start = 16000\n",
    "\n",
    "def apply_log(x): return np.log(x + 1.0)\n",
    "def apply_exp(x): return np.exp(x) - 1.0\n",
    "\n",
    "for x,y,n in pull_examples(target_file=local_path+'\\\\'+source, \n",
    "                           vectorizer=std_row, min_max=min_max, sparse = False,\n",
    "                           binary_features=b_vars, numeric_features=n_vars, target='cnt'):\n",
    "    y_log = apply_log(y)\n",
    "    # Extract only quantitative features and expand them\n",
    "    num_index = [j for j, i in enumerate(std_row.feature_names_) if i in n_vars]\n",
    "    x_poly = poly.fit_transform(x[:,num_index])[:,len(num_index):]\n",
    "    new_x = np.concatenate((x, x_poly), axis=1)\n",
    "    \n",
    "    # MACHINE LEARNING\n",
    "    if (n+1) >= predictions_start:\n",
    "        # HOLDOUT AFTER N PHASE\n",
    "        predicted = SGD.predict(new_x)\n",
    "        val_rmse += (apply_exp(predicted) - y)**2\n",
    "        val_rmsle += (predicted - y_log)**2\n",
    "        if (n-predictions_start+1) % 250 == 0 and (n+1) > predictions_start:\n",
    "            print n,\n",
    "            print '%s holdout RMSE: %0.3f' % (time.strftime('%X'), (val_rmse / float(n-predictions_start+1))**0.5),\n",
    "            print 'holdout RMSLE: %0.3f' % ((val_rmsle / float(n-predictions_start+1))**0.5)\n",
    "    else:\n",
    "        # LEARNING PHASE\n",
    "        SGD.partial_fit(new_x, y_log)\n",
    "print '%s FINAL holdout RMSE: %0.3f' % (time.strftime('%X'), (val_rmse / float(n-predictions_start+1))**0.5)\n",
    "print '%s FINAL holdout RMSLE: %0.3f' % (time.strftime('%X'), (val_rmsle / float(n-predictions_start+1))**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying explicit high dimensional mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:45:07 Progressive accuracy at example 5000: 0.621\n",
      "09:45:24 Progressive accuracy at example 10000: 0.653\n",
      "09:45:41 Progressive accuracy at example 15000: 0.670\n",
      "09:45:59 Progressive accuracy at example 20000: 0.681\n",
      "09:46:17 Progressive accuracy at example 25000: 0.689\n",
      "09:46:35 Progressive accuracy at example 30000: 0.695\n",
      "09:46:53 Progressive accuracy at example 35000: 0.699\n",
      "09:47:12 Progressive accuracy at example 40000: 0.704\n",
      "09:47:30 Progressive accuracy at example 45000: 0.706\n",
      "09:47:49 Progressive accuracy at example 50000: 0.709\n"
     ]
    }
   ],
   "source": [
    "source = 'shuffled_covtype.data'\n",
    "local_path = os.getcwd()\n",
    "n_vars = ['var_'+str(j) for j in range(54)]\n",
    "std_row, min_max = explore(target_file=local_path+'\\\\'+source, binary_features=list(), \n",
    "                  fieldnames= n_vars+['covertype'], numeric_features=n_vars, max_rows=50000)\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "SGD = SGDClassifier(loss='hinge', penalty=None, random_state=1, average=True)\n",
    "rbf_feature = RBFSampler(gamma=0.5, n_components=300, random_state=0)\n",
    "accuracy = 0\n",
    "accuracy_record = list()\n",
    "predictions_start = 50\n",
    "sample = 5000\n",
    "early_stop = 50000\n",
    "for x,y,n in pull_examples(target_file=local_path+'\\\\'+source, \n",
    "                           vectorizer=std_row,\n",
    "                           min_max=min_max,\n",
    "                           binary_features=list(),\n",
    "                           numeric_features=n_vars, \n",
    "                           fieldnames= n_vars+['covertype'], target='covertype', sparse=False):\n",
    "    rbf_x = rbf_feature.fit_transform(x)\n",
    "    # LEARNING PHASE\n",
    "    if n > predictions_start:\n",
    "        accuracy += int(int(SGD.predict(rbf_x))==y[0])\n",
    "        if n % sample == 0:\n",
    "            accuracy_record.append(accuracy / float(sample))\n",
    "            print '%s Progressive accuracy at example %i: %0.3f' % (time.strftime('%X'), \\\n",
    "               n, np.mean(accuracy_record[-sample:]))\n",
    "            accuracy = 0\n",
    "    if early_stop and n >= early_stop:\n",
    "            break\n",
    "    SGD.partial_fit(rbf_x, y, classes=range(1,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'penalty': 'l2', 'alpha': 0.001\n",
      "Iteration 1 - RMSE: 216.170 - RMSE: 1.440\n",
      "Iteration 20 - RMSE: 152.175 - RMSE: 0.857\n",
      "\n",
      "'penalty': 'l2', 'alpha': 0.0001\n",
      "Iteration 1 - RMSE: 714.071 - RMSE: 4.096\n",
      "Iteration 31 - RMSE: 184.677 - RMSE: 1.053\n",
      "\n",
      "'penalty': 'l1', 'alpha': 0.01\n",
      "Iteration 1 - RMSE: 1050.809 - RMSE: 6.044\n",
      "Iteration 36 - RMSE: 225.036 - RMSE: 1.298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.grid_search import ParameterSampler\n",
    "\n",
    "source = '\\\\bikesharing\\\\hour.csv'\n",
    "local_path = os.getcwd()\n",
    "b_vars = ['holiday','hr','mnth', 'season','weathersit','weekday','workingday','yr']\n",
    "n_vars = ['hum', 'temp', 'atemp', 'windspeed']\n",
    "std_row, min_max = explore(target_file=local_path+'\\\\'+source, binary_features=b_vars, numeric_features=n_vars)\n",
    "\n",
    "val_rmse = 0\n",
    "val_rmsle = 0\n",
    "predictions_start = 16000\n",
    "tmp_rsmle = 10**6\n",
    "\n",
    "def apply_log(x): return np.log(x + 1.0)\n",
    "def apply_exp(x): return np.exp(x) - 1.0\n",
    "\n",
    "param_grid = {'penalty':['l1', 'l2'], 'alpha': 10.0**-np.arange(2,5)}\n",
    "random_tests = 3\n",
    "search_schedule = list(ParameterSampler(param_grid, n_iter=random_tests, random_state=5))\n",
    "results = dict()\n",
    "\n",
    "for search in search_schedule:\n",
    "    SGD = SGDRegressor(loss='epsilon_insensitive', epsilon=0.001, penalty=None, random_state=1, average=True)\n",
    "    params =SGD.get_params()\n",
    "    new_params = {p:params[p] if p not in search else search[p] for p in params}\n",
    "    SGD.set_params(**new_params)\n",
    "    print str(search)[1:-1]\n",
    "    for iterations in range(200):\n",
    "        for x,y,n in pull_examples(target_file=local_path+'\\\\'+source, \n",
    "                                   vectorizer=std_row, min_max=min_max, sparse = False,\n",
    "                                   binary_features=b_vars, numeric_features=n_vars, target='cnt'):\n",
    "            y_log = apply_log(y)\n",
    "\n",
    "            # MACHINE LEARNING\n",
    "            if (n+1) >= predictions_start:\n",
    "                # HOLDOUT AFTER N PHASE\n",
    "                predicted = SGD.predict(x)\n",
    "                val_rmse += (apply_exp(predicted) - y)**2\n",
    "                val_rmsle += (predicted - y_log)**2\n",
    "            else:\n",
    "                # LEARNING PHASE\n",
    "                SGD.partial_fit(x, y_log)\n",
    "\n",
    "        examples = float(n-predictions_start+1) * (iterations+1)\n",
    "        print_rmse = (val_rmse / examples)**0.5\n",
    "        print_rmsle = (val_rmsle / examples)**0.5\n",
    "        if iterations == 0:\n",
    "            print 'Iteration %i - RMSE: %0.3f - RMSE: %0.3f' % (iterations+1, print_rmse, print_rmsle)\n",
    "        if iterations > 0:\n",
    "            if tmp_rmsle / print_rmsle <= 1.01:\n",
    "                print 'Iteration %i - RMSE: %0.3f - RMSE: %0.3f\\n' % (iterations+1, print_rmse, print_rmsle)\n",
    "                results[str(search)]= {'rmse':float(print_rmse), 'rmsle':float(print_rmsle)}\n",
    "                break\n",
    "        tmp_rmsle = print_rmsle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other alternatives for SVM fast learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Useful dataset examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 | price:.23 sqft:.25 age:.05 2006\n",
      "1 2 'second_house | price:.18 sqft:.15 age:.35 1976\n",
      "0 1 0.5 'third_house | price:.53 sqft:.32 age:.87 1924\n"
     ]
    }
   ],
   "source": [
    "with open('house_dataset','wb') as W:\n",
    "    W.write(\"0 | price:.23 sqft:.25 age:.05 2006\\n\")\n",
    "    W.write(\"1 2 'second_house | price:.18 sqft:.15 age:.35 1976\\n\")\n",
    "    W.write(\"0 1 0.5 'third_house | price:.53 sqft:.32 age:.87 1924\\n\")\n",
    "\n",
    "with open('house_dataset','rb') as R:\n",
    "    for line in R:\n",
    "        print line.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###A way to call VW from Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def execute_vw(parameters):\n",
    "    execution = subprocess.Popen('vw '+parameters, \\\n",
    "       shell=True, stderr=subprocess.PIPE)\n",
    "    line = \"\"\n",
    "    history = \"\"\n",
    "    while True:\n",
    "        out = execution.stderr.read(1)\n",
    "        history += out\n",
    "        if out == '' and execution.poll() != None:\n",
    "            print '------------ COMPLETED ------------\\n'\n",
    "            break\n",
    "        if out != '':\n",
    "            line += out\n",
    "            if '\\n' in line[-2:]:\n",
    "                print line[:-2]\n",
    "                line = ''\n",
    "    return history.split('\\r\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = house_dataset\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0   0.0000   0.0000        5\n",
      "0.666667 1.000000            2            3.0   1.0000   0.0000        5\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 3\n",
      "passes used = 1\n",
      "weighted example sum = 4.000000\n",
      "weighted label sum = 2.000000\n",
      "average loss = 0.750000\n",
      "best constant = 0.500000\n",
      "best constant's loss = 0.250000\n",
      "total feature number = 15\n",
      "------------ COMPLETED ------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = \"house_dataset\"\n",
    "results = execute_vw(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Processing examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def vw_convert(origin_file, target_file, binary_features, numeric_features, target, transform_target=lambda(x):x,\n",
    "               separator=',', classification=True, multiclass=False, fieldnames= None, header=True, sparse=True):\n",
    "    \"\"\"\n",
    "    Reads a online style stream and returns a generator of normalized feature vectors\n",
    "    \n",
    "    Parameters\n",
    "    ‐‐‐‐‐‐‐‐‐‐\n",
    "    original_file = the csv file you are taken the data from \n",
    "    target file = the file to stream from\n",
    "    binary_features = the list of qualitative features to consider\n",
    "    numeric_features = the list of numeric features to consider\n",
    "    target = the label of the response variable\n",
    "    transform_target = a function transforming the response\n",
    "    separator = the field separator character\n",
    "    classification = a Boolean indicating if it is classification\n",
    "    multiclass =  a Boolean for multiclass classification\n",
    "    fieldnames = the fields' labels (can be ommitted and read from file)\n",
    "    header = a boolean indicating if the original file has an header\n",
    "    sparse = if a sparse vector is to be returned from the generator\n",
    "    \"\"\"\n",
    "    with open(target_file, 'wb') as W:\n",
    "        with open(origin_file, 'rb') as R:\n",
    "            iterator = csv.DictReader(R, fieldnames, delimiter=separator)\n",
    "            for n, row in enumerate(iterator):\n",
    "                if not header or n>0:\n",
    "                # DATA PROCESSING\n",
    "                    response = transform_target(float(row[target]))\n",
    "                    if classification and not multiclass:\n",
    "                            if response == 0:\n",
    "                                stream_row = '-1 '\n",
    "                            else:\n",
    "                                stream_row = '1 '\n",
    "                    else:\n",
    "                        stream_row = str(response)+' '\n",
    "                    quantitative = list()\n",
    "                    qualitative  = list()\n",
    "                    for k,v in row.iteritems():\n",
    "                        if k in binary_features:\n",
    "                            qualitative.append(str(k)+'_'+str(v)+':1')\n",
    "                        else:\n",
    "                            if k in numeric_features and (float(v)!=0 or not sparse):\n",
    "                                quantitative.append(str(k)+':'+str(v))\n",
    "                    if quantitative:\n",
    "                        stream_row += '|n '+' '.join(quantitative)\n",
    "                    if qualitative:\n",
    "                        stream_row += '|q ' + ' '.join(qualitative)\n",
    "                    W.write(stream_row+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Examples with toys datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris, load_boston\n",
    "from random import seed\n",
    "iris = load_iris()\n",
    "seed(2)\n",
    "re_order = np.random.permutation(len(iris.target))\n",
    "with open('iris_versicolor.vw','wb') as W1:\n",
    "    for k in re_order:\n",
    "        y = iris.target[k]\n",
    "        X = iris.values()[1][k,:]\n",
    "        features = ' |f '+' '.join([a+':'+str(b) for a,b in zip(map(lambda(a): a[:-5].replace(' ','_'), iris.feature_names),X)])\n",
    "        target = '1' if y==1 else '-1'\n",
    "        W1.write(target+features+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "seed(2)\n",
    "re_order = np.random.permutation(len(boston.target))\n",
    "with open('boston.vw','wb') as W1:\n",
    "     for k in re_order:\n",
    "        y = boston.target[k]\n",
    "        X = boston.data[k,:]\n",
    "        features = ' |f '+' '.join([a+':'+str(b) for a,b in zip(map(lambda(a): a[:-5].replace(' ','_'), iris.feature_names),X)])\n",
    "        W1.write(str(y)+features+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Binary Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using l2 regularization = 1e-006\n",
      "predictions = iris_bin.test\n",
      "Lambda = 1e-006\n",
      "Kernel = rbf\n",
      "bandwidth = 0.1\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "using no cache\n",
      "Reading datafile = iris_versicolor.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0  -1.0000   0.0000        5\n",
      "0.502494 0.004988            2            2.0  -1.0000  -0.9950        5\n",
      "0.498283 0.494072            4            4.0  -1.0000  -0.9872        5\n",
      "0.942084 1.385885            8            8.0   1.0000   0.2481        5\n",
      "0.651197 0.360311           16           16.0  -1.0000  -0.2290        5\n",
      "0.518579 0.385960           32           32.0   1.0000   0.0946        5\n",
      "0.428142 0.337705           64           64.0   1.0000   2.0354        5\n",
      "0.321869 0.215597          128          128.0  -1.0000  -2.1214        5\n",
      "\n",
      "finished run\n",
      "number of examples = 150\n",
      "weighted example sum = 150.000000\n",
      "weighted label sum = -50.000000\n",
      "average loss = 0.327921\n",
      "best constant = -0.333333\n",
      "best constant's loss = 0.888889\n",
      "total feature number = 750\n",
      "Num support = 57\n",
      "Number of kernel evaluations = 9927 Number of cache queries = 19500\n",
      "Total loss = 49.188114\n",
      "Done freeing model\n",
      "Done freeing kernel params\n",
      "Done with finish \n",
      "------------ COMPLETED ------------\n",
      "\n",
      "holdout accuracy: 0.931\n"
     ]
    }
   ],
   "source": [
    "params = '--ksvm --l2 0.000001 --reprocess 2 -b 18 --kernel rbf --bandwidth=0.1 -p iris_bin.test -d iris_versicolor.vw'\n",
    "results = execute_vw(params)\n",
    "\n",
    "accuracy = 0\n",
    "with open('iris_bin.test', 'rb') as R:\n",
    "    with open('iris_versicolor.vw', 'rb') as TRAIN:\n",
    "        holdouts = 0.0\n",
    "        for n,(line, example) in enumerate(zip(R,TRAIN)):\n",
    "            if (n+1) % 10==0:\n",
    "                predicted = float(line.strip())\n",
    "                y = float(example.split('|')[0])\n",
    "                accuracy += np.sign(predicted)==np.sign(y)\n",
    "                holdouts += 1            \n",
    "print 'holdout accuracy: %0.3f' % ((accuracy / holdouts)**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Boston dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = boston.model\n",
      "using dropout for neural network training\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = cache_train.vw\n",
      "Reading datafile = boston.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "416.159973 416.159973            1            1.0  20.4000   0.0000        3\n",
      "468.479584 520.799194            2            2.0  24.1000   1.2790        4\n",
      "368.088074 267.696564            4            4.0  18.9000   3.6200        4\n",
      "371.617607 375.147141            8            8.0  19.8000   7.9455        3\n",
      "303.415839 235.214072           16           16.0  20.6000   7.1648        3\n",
      "261.152876 218.889912           32           32.0  19.6000  12.2380        4\n",
      "237.683406 214.213937           64           64.0  21.7000  12.8101        4\n",
      "166.570917 95.458427          128          128.0  21.4000  14.5432        3\n",
      "161.194035 155.817154          256          256.0  15.2000  21.8946        3\n",
      "146.445487 146.445487          512          512.0  25.1000  26.1432        4 h\n",
      "116.584593 86.723700         1024         1024.0  36.4000  27.2230        4 h\n",
      "102.224233 87.863872         2048         2048.0  15.1000  14.9938        3 h\n",
      "82.824130 63.510250         4096         4096.0  20.0000  22.5974        3 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 456\n",
      "passes used = 12\n",
      "weighted example sum = 5472.000000\n",
      "weighted label sum = 122298.001917\n",
      "average loss = 58.793495 h\n",
      "best constant = 22.349781\n",
      "total feature number = 18240\n",
      "------------ COMPLETED ------------\n",
      "\n",
      "only testing\n",
      "predictions = boston.test\n",
      "using dropout for neural network testing\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "creating cache_file = cache_test.vw\n",
      "Reading datafile = boston.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.953481 0.953481            1            1.0  20.4000  21.3765        3\n",
      "4.828651 8.703821            2            2.0  24.1000  27.0502        4\n",
      "20.278078 35.727504            4            4.0  18.9000  26.8244        4\n",
      "20.755837 21.233596            8            8.0  19.8000  20.2893        3\n",
      "30.424801 40.093766           16           16.0  16.2000  18.4288        3\n",
      "70.045755 109.666709           32           32.0  14.1000  16.3200        3\n",
      "68.723481 67.401208           64           64.0  25.0000  20.3249        3\n",
      "58.814945 48.906409          128          128.0  24.8000  23.2054        4\n",
      "77.400475 95.986005          256          256.0  13.6000  21.1639        3\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 506\n",
      "passes used = 1\n",
      "weighted example sum = 506.000000\n",
      "weighted label sum = 11401.600174\n",
      "average loss = 65.186000\n",
      "best constant = 22.532808\n",
      "total feature number = 1687\n",
      "------------ COMPLETED ------------\n",
      "\n",
      "holdout RMSE: 7.967\n"
     ]
    }
   ],
   "source": [
    "params = 'boston.vw -f boston.model --loss_function squared -k --cache_file cache_train.vw --passes=20 --nn 5 --dropout'\n",
    "results = execute_vw(params)\n",
    "params = '-t boston.vw -i boston.model -k --cache_file cache_test.vw -p boston.test'\n",
    "results = execute_vw(params)\n",
    "val_rmse = 0\n",
    "with open('boston.test', 'rb') as R:\n",
    "    with open('boston.vw', 'rb') as TRAIN:\n",
    "        holdouts = 0.0\n",
    "        for n,(line, example) in enumerate(zip(R,TRAIN)):\n",
    "            if (n+1) % 10==0:\n",
    "                predicted = float(line.strip())\n",
    "                y = float(example.split('|')[0])\n",
    "                val_rmse += (predicted - y)**2\n",
    "                holdouts += 1            \n",
    "print 'holdout RMSE: %0.3f' % ((val_rmse / holdouts)**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster bikesharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def apply_log(x): \n",
    "    return np.log(x + 1.0)\n",
    "\n",
    "def apply_exp(x): \n",
    "    return np.exp(x) - 1.0\n",
    "\n",
    "local_path = os.getcwd()\n",
    "b_vars = ['holiday','hr','mnth', 'season','weathersit','weekday','workingday','yr']\n",
    "n_vars = ['hum', 'temp', 'atemp', 'windspeed']\n",
    "source = '\\\\bikesharing\\\\hour.csv'\n",
    "origin = target_file=local_path+'\\\\'+source\n",
    "target = target_file=local_path+'\\\\'+'bike.vw'\n",
    "vw_convert(origin, target, binary_features=b_vars, numeric_features=n_vars, target = 'cnt', transform_target=apply_log,\n",
    "               separator=',', classification=False, multiclass=False, fieldnames= None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_regressor = regression.model\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = cache_train.vw\n",
      "Reading datafile = bike.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "13.790617 13.790617            1            1.0   3.7136   0.0000       12\n",
      "9.376448 4.962280            2            2.0   3.4965   1.2689       12\n",
      "5.311779 1.247110            4            4.0   0.6931   2.1128       12\n",
      "2.844452 0.377125            8            8.0   2.1972   1.4237       12\n",
      "2.372758 1.901063           16           16.0   4.5433   3.9948       13\n",
      "1.423223 0.473688           32           32.0   3.0445   1.8111       13\n",
      "1.383102 1.342980           64           64.0   4.7095   3.7816       12\n",
      "1.387249 1.391397          128          128.0   4.2627   4.1522       13\n",
      "1.165221 0.943192          256          256.0   1.9459   3.1925       13\n",
      "0.954876 0.744530          512          512.0   4.6052   3.5848       13\n",
      "0.761389 0.567903         1024         1024.0   4.7095   4.2340       13\n",
      "0.587091 0.412793         2048         2048.0   2.1972   1.3523       13\n",
      "0.455539 0.323988         4096         4096.0   5.4381   5.4561       13\n",
      "0.394745 0.333951         8192         8192.0   2.8904   3.4612       12\n",
      "0.558222 0.558222        16384        16384.0   4.5433   4.0472       13 h\n",
      "0.506689 0.455156        32768        32768.0   4.4188   4.4544       13 h\n",
      "0.466235 0.425781        65536        65536.0   4.6052   4.3960       13 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 15999\n",
      "passes used = 7\n",
      "weighted example sum = 111993.000000\n",
      "weighted label sum = 512395.632496\n",
      "average loss = 0.425561 h\n",
      "best constant = 4.575247\n",
      "total feature number = 1441888\n",
      "------------ COMPLETED ------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = 'bike.vw -f regression.model -k --cache_file cache_train.vw --passes=100 --hash strings --holdout_after 16000'\n",
    "results = execute_vw(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only testing\n",
      "predictions = pred.test\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "creating cache_file = cache_test.vw\n",
      "Reading datafile = bike.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.481086 1.481086            1            1.0   3.7136   2.4966       12\n",
      "1.857857 2.234629            2            2.0   3.4965   2.0016       12\n",
      "1.327467 0.797078            4            4.0   0.6931   1.2289       12\n",
      "3.693974 6.060481            8            8.0   2.1972   5.0060       12\n",
      "2.206792 0.719610           16           16.0   4.5433   5.0346       13\n",
      "2.154553 2.102313           32           32.0   3.0445   4.6435       13\n",
      "1.337130 0.519708           64           64.0   4.7095   4.7743       12\n",
      "0.789608 0.242085          128          128.0   4.2627   4.5554       13\n",
      "0.642582 0.495556          256          256.0   1.9459   2.2025       13\n",
      "0.572553 0.502523          512          512.0   4.6052   4.2045       13\n",
      "0.484620 0.396687         1024         1024.0   4.7095   4.4613       13\n",
      "0.445152 0.405684         2048         2048.0   2.1972   1.9334       13\n",
      "0.382539 0.319926         4096         4096.0   5.4381   5.5692       13\n",
      "0.373219 0.363899         8192         8192.0   2.8904   3.3176       12\n",
      "0.356879 0.340539        16384        16384.0   5.1761   5.1955       13\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 17378\n",
      "passes used = 1\n",
      "weighted example sum = 17378.000000\n",
      "weighted label sum = 79501.549026\n",
      "average loss = 0.363518\n",
      "best constant = 4.574839\n",
      "total feature number = 223711\n",
      "------------ COMPLETED ------------\n",
      "\n",
      "holdout RMSE: 106.334\n",
      "holdout RMSLE: 0.653\n"
     ]
    }
   ],
   "source": [
    "params = '-t bike.vw -i regression.model -k --cache_file cache_test.vw -p pred.test'\n",
    "results = execute_vw(params)\n",
    "val_rmse = 0\n",
    "val_rmsle = 0\n",
    "with open('pred.test', 'rb') as R:\n",
    "    with open('bike.vw', 'rb') as TRAIN:\n",
    "        holdouts = 0.0\n",
    "        for n,(line, example) in enumerate(zip(R,TRAIN)):\n",
    "            if n > 16000:\n",
    "                predicted = float(line.strip())\n",
    "                y_log = float(example.split('|')[0])\n",
    "                y = apply_exp(y_log)\n",
    "                val_rmse += (apply_exp(predicted) - y)**2\n",
    "                val_rmsle += (predicted - y_log)**2\n",
    "                holdouts += 1\n",
    "            \n",
    "print 'holdout RMSE: %0.3f' % ((val_rmse / holdouts)**0.5)\n",
    "print 'holdout RMSLE: %0.3f' % ((val_rmsle / holdouts)**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covertype dataset crunched by VW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating cubic features for triples: nnn \n",
      "final_regressor = multiclass.model\n",
      "Num weight bits = 18\n",
      "learning rate = 1\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "decay_learning_rate = 1\n",
      "creating cache_file = cache_train.vw\n",
      "Reading datafile = covtype.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "1.000000 1.000000            1            1.0        2        1      377\n",
      "1.000000 1.000000            2            2.0        1        2      377\n",
      "0.750000 0.500000            4            4.0        2        2      377\n",
      "0.750000 0.750000            8            8.0        6        1      377\n",
      "0.687500 0.625000           16           16.0        2        2      377\n",
      "0.531250 0.375000           32           32.0        2        3      377\n",
      "0.484375 0.437500           64           64.0        1        1      377\n",
      "0.468750 0.453125          128          128.0        2        1      377\n",
      "0.464844 0.460938          256          256.0        2        1      377\n",
      "0.460938 0.457031          512          512.0        2        2      377\n",
      "0.447266 0.433594         1024         1024.0        3        3      377\n",
      "0.410645 0.374023         2048         2048.0        2        2      377\n",
      "0.389160 0.367676         4096         4096.0        2        2      231\n",
      "0.364990 0.340820         8192         8192.0        2        2      377\n",
      "0.341736 0.318481        16384        16384.0        3        3      377\n",
      "0.316742 0.291748        32768        32768.0        2        2      377\n",
      "0.294235 0.271729        65536        65536.0        2        2      377\n",
      "0.277489 0.260742       131072       131072.0        2        2      377\n",
      "0.263508 0.249527       262144       262144.0        1        2      377\n",
      "0.254219 0.254219       524288       524288.0        1        2      377 h\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 522911\n",
      "passes used = 2\n",
      "weighted example sum = 1045822.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.235831 h\n",
      "total feature number = 384854530\n",
      "------------ COMPLETED ------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "local_path = os.getcwd()\n",
    "n_vars = ['var_'+'0'*int(j<10)+str(j) for j in range(54)]\n",
    "source = 'shuffled_covtype.data'\n",
    "origin = target_file=local_path+'\\\\'+source\n",
    "target = target_file=local_path+'\\\\'+'covtype.vw'\n",
    "vw_convert(origin, target, binary_features=list(), fieldnames= n_vars+['covertype'], numeric_features=n_vars,\n",
    "    target = 'covertype', separator=',', classification=True, multiclass=True, header=False, sparse=False)\n",
    "params = 'covtype.vw --ect 7 -f multiclass.model -k --cache_file cache_train.vw --passes=2 -l 1.0 --cubic nnn'\n",
    "results = execute_vw(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating cubic features for triples: nnn \n",
      "only testing\n",
      "predictions = covertype.test\n",
      "Num weight bits = 18\n",
      "learning rate = 0.5\n",
      "initial_t = 0\n",
      "power_t = 0.5\n",
      "creating cache_file = cache_test.vw\n",
      "Reading datafile = covtype.vw\n",
      "num sources = 1\n",
      "average  since         example        example  current  current  current\n",
      "loss     last          counter         weight    label  predict features\n",
      "0.000000 0.000000            1            1.0        2        2      377\n",
      "0.500000 1.000000            2            2.0        1        2      377\n",
      "0.250000 0.000000            4            4.0        2        2      377\n",
      "0.125000 0.000000            8            8.0        6        6      377\n",
      "0.250000 0.375000           16           16.0        2        1      377\n",
      "0.187500 0.125000           32           32.0        1        2      377\n",
      "0.265625 0.343750           64           64.0        1        1      377\n",
      "0.226563 0.187500          128          128.0        2        2      377\n",
      "0.265625 0.304688          256          256.0        7        1      377\n",
      "0.251953 0.238281          512          512.0        3        3      377\n",
      "0.250000 0.248047         1024         1024.0        2        2      377\n",
      "0.241211 0.232422         2048         2048.0        2        2      377\n",
      "0.238525 0.235840         4096         4096.0        2        2      377\n",
      "0.232544 0.226563         8192         8192.0        2        2      377\n",
      "0.233398 0.234253        16384        16384.0        2        2      377\n",
      "0.229889 0.226379        32768        32768.0        3        3      377\n",
      "0.229568 0.229248        65536        65536.0        6        6      377\n",
      "0.230247 0.230927       131072       131072.0        5        2      377\n",
      "0.230877 0.231506       262144       262144.0        3        3      377\n",
      "0.231508 0.232140       524288       524288.0        1        2      377\n",
      "\n",
      "finished run\n",
      "number of examples per pass = 581012\n",
      "passes used = 1\n",
      "weighted example sum = 581012.000000\n",
      "weighted label sum = 0.000000\n",
      "average loss = 0.231179\n",
      "total feature number = 213797603\n",
      "------------ COMPLETED ------------\n",
      "\n",
      "holdout accuracy: 0.768\n"
     ]
    }
   ],
   "source": [
    "params = '-t covtype.vw -i multiclass.model -k --cache_file cache_test.vw -p covertype.test'\n",
    "results = execute_vw(params)\n",
    "accuracy = 0\n",
    "with open('covertype.test', 'rb') as R:\n",
    "    with open('covtype.vw', 'rb') as TRAIN:\n",
    "        holdouts = 0.0\n",
    "        for n,(line, example) in enumerate(zip(R,TRAIN)):\n",
    "            if (n+1) % 10==0:\n",
    "                predicted = float(line.strip())\n",
    "                y = float(example.split('|')[0])\n",
    "                accuracy += predicted ==y\n",
    "                holdouts += 1\n",
    "print 'holdout accuracy: %0.3f' % (accuracy / holdouts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
